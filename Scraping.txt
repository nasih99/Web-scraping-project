import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin
import pandas as pd


BASE_URL = "https://books.toscrape.com/"

def extract_books(soup, current_page):
    books = []
    all_books = soup.select('article.product_pod')
    
    for book in all_books:
        title = book.h3.a['title']
        price = book.select_one('p.price_color').text
        rating = book.select_one('p.star-rating')['class'][1]
        availability = book.select_one('p.instock.availability').text.strip()

        # ✅ Join book URL using the current page
        book_link = book.h3.a['href']
        book_url = urljoin(current_page, book_link)
        genre = get_genre(book_url)

        books.append({
            'Title': title,
            'Price': price,
            'Rating': rating,
            'Availability': availability,
            'Genre': genre
        })
    
    return books



def scrape_all_books():
    all_books = []
    current_page = BASE_URL

    while True:
        print("Scraping:", current_page)
        response = requests.get(current_page)
        soup = BeautifulSoup(response.text, 'html.parser')

        books = extract_books(soup, current_page)
        all_books.extend(books)

        next_button = soup.select_one('li.next a')
        if next_button:
            next_url = next_button['href']
            current_page = urljoin(current_page, next_url)
        else:
            break

    return all_books



books_data = scrape_all_books()
df = pd.DataFrame(books_data)
df.head()






df.to_csv('books.csv', index=False)
print(f"Saved {len(df)} books to 'books.csv'")





from pyspark.sql import SparkSession

# create Spark session
spark = SparkSession.builder \
    .appName("Books Data Analysis") \
    .getOrCreate()

# load CSV file into DataFrame
df = spark.read.option("header", True).option("encoding", "UTF-8").csv("books.csv")


df.printSchema()


df.show(5)


df.describe().show()


# import required functions
from pyspark.sql.functions import col, when, regexp_replace

#  Remove '£' from Price and convert it to float
df = df.withColumn("Price", regexp_replace("Price", "£", ""))
df = df.withColumn("Price", col("Price").cast("float"))

#  Show books with Price greater than 20
df.filter(col("Price") > 20).show()

#  convert Rating words to numbers
df = df.withColumn(
    "RatingNum",
    when(col("Rating") == "One", 1)
    .when(col("Rating") == "Two", 2)
    .when(col("Rating") == "Three", 3)
    .when(col("Rating") == "Four", 4)
    .when(col("Rating") == "Five", 5)
)

#  show books with rating 4 or more
df.filter(col("RatingNum") >= 4).show()






